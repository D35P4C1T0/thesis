\section{\textit{Benchmarking} Automatizzato per il Confronto Prestazionale tra Librerie}

Al fine di condurre un'analisi comparativa sistematica delle prestazioni delle librerie di visualizzazione cartografica, è stato sviluppato un progetto dedicato al benchmarking automatizzato. Questo sistema acquisisce misurazioni quantitative e ripetibili dei tempi di esecuzione e rendering, riducendo al minimo l'intervento manuale e le potenziali distorsioni. Ogni prova è stata eseguita in condizioni operative il più possibile isolate, evitando interferenze da stati precedenti o cache.  
I dati raccolti hanno permesso di valutare la reattività di ciascuna libreria e presentare un'analisi concisa e fruibile.

L'architettura di test si basa su un'istanza Flask che espone una pagina web tramite l'endpoint \texttt{/benchmark}.  
Si specifica che \textit{Folium} non è incluso nelle misurazioni: essendo server-side rendered (SSR), il confronto diretto con librerie client-side rendered (CSR) non sarebbe significativo.  
In dettaglio:

\begin{itemize}
    \item \textbf{Server-Side Rendering (SSR)}: il contenuto HTML completo viene generato sul server e inviato al client, garantendo visualizzazione immediata e migliore indicizzazione \cite{peerdh-ssr-csr-comparison}.
    \item \textbf{Client-Side Rendering (CSR)}: il server invia solo HTML minimo, e il browser costruisce dinamicamente l'interfaccia via JavaScript, con possibile ritardo nella visualizzazione iniziale \cite{devto-csr-vs-ssr}.
\end{itemize}

  

Il processo di benchmarking si articola nelle seguenti fasi principali:

\begin{itemize}[leftmargin=*]
    \item \textbf{Caricamento iterativo delle mappe:}  
    Per ciascuna libreria e iterazione, la pagina web dedicata (\texttt{/leaflet}, \texttt{/openlayers}, ecc.) viene caricata in un \texttt{<iframe>} integrato nella pagina di \textit{benchmark}. Questo crea un ambiente \textit{sandboxed}, isolando ogni test e riducendo interferenze tra iterazioni. L'\textit{iframe} può essere reso invisibile o visibile per \textit{debug}.

    \item \textbf{Strategie anti-cache:}  
    Per garantire caricamenti freschi delle risorse, sono state adottate due misure complementari:
    \begin{itemize}
        \item \textit{Direttive server-side:} Flask imposta intestazioni HTTP come \texttt{Cache-Control: no-cache, no-store, must-revalidate}, \texttt{Pragma: no-cache} ed \texttt{Expires: 0} per forzare il ricaricamento dei contenuti.
        \item \textit{Cache busting client-side:} Ogni URL di mappa nell'\textit{iframe} riceve un parametro di query univoco basato sul \textit{timestamp} (es. \texttt{?t=xxxxxxxxxxxx}) per prevenire il riuso di risorse memorizzate.
    \end{itemize}

    \item \textbf{Raccolta asincrona delle metriche:}  
    Ogni pagina mappa misura i tempi chiave (recupero dati, inizializzazione, rendering) tramite \texttt{performance.now()}. Al termine, emette un evento JavaScript \texttt{benchmarkMetricsReady} con un \texttt{payload} JSON contenente tutte le metriche (\texttt{dataFetchTime\_ms}, \texttt{mapInitAndHeatmapRenderTime\_ms}, \texttt{totalClientLoadTime\_ms}, \texttt{totalPoints}). La pagina genitore cattura l'evento e registra i dati.

    \item \textbf{Aggregazione e persistenza dei dati:}  
    Le metriche di ogni iterazione vengono accumulate in un array JavaScript con metadati contestuali (nome libreria, numero iterazione, User-Agent). Al termine, l'intero dataset viene serializzato in JSON e inviato a un endpoint Flask (\texttt{/save\_benchmark\_data}), che lo salva in formato CSV (\texttt{benchmarks.csv}). La tabella risultante, esemplificata in Tabella \ref{tab:csv-metriche-mappe}, consente analisi comparative e importazione in software statistici.
\end{itemize}


\begin{table}[ht]
\centering
\caption{Esempio di dati raccolti per \textit{Leaflet}} % Added descriptive caption
\label{tab:csv-metriche-mappe}
\sisetup{
  output-exponent-marker=\text{\,E\,},
  exponent-product={},
  group-digits=false
}
\begin{tabular}{
  l
  S[table-format=1.0]
  S[table-format=5.0]
  S[table-format=3.2]
  S[table-format=3.0]
}
\toprule
\textbf{Mappa} & \textbf{Iterazioni} & \textbf{Punti totali} & \textbf{Data Fetch (ms)} & \textbf{Map Init \& Render (ms)} \\
\midrule
Leaflet & 1 & 76533 & 502.75 & 580 \\
Leaflet & 2 & 76533 & 487.70 & 561 \\
Leaflet & 3 & 76533 & 480.63 & 546 \\
Leaflet & 4 & 76533 & 468.64 & 536 \\
Leaflet & 5 & 76533 & 462.73 & 539 \\
\vdots  &\vdots  &\vdots  &\vdots  &\vdots \\
Leaflet & 46 & 76533 & 455,67 & 526 \\
Leaflet & 47 & 76533 & 508,65 & 577 \\
Leaflet & 48 & 76533 & 594.60 & 658 \\
Leaflet & 49 & 76533 & 456.62 & 520 \\ 
Leaflet & 50 & 76533 & 499.65 & 568 \\
\bottomrule
\end{tabular}
\end{table}


% focus sul benchmark
\subsection{Metodologia di Rilevamento delle Metriche}

Il sistema di \textit{benchmarking} automatizzato esegue 50 iterazioni per ciascuna libreria cartografica. In ogni iterazione, vengono misurate tre metriche principali: il tempo di recupero dei dati (\textit{data fetch time}), il tempo di inizializzazione e \textit{rendering} della mappa (\textit{map initialization and heatmap render time}) e il tempo totale di caricamento (\textit{total client load time}). Il processo di misurazione inizia quando l'utente richiede la visualizzazione della mappa e termina quando la \textit{heatmap} è completamente renderizzata e interattiva. Per ogni libreria, è stato implementato un sistema di eventi personalizzato che marca con precisione i momenti chiave del ciclo di vita della mappa:

\begin{itemize}
    \item \textbf{Tempo di Recupero Dati}: Misurato dall'inizio della richiesta HTTP fino al completamento del download dei dati dei punti. Questo intervallo è calcolato utilizzando l'API Performance del browser, specificamente attraverso il \texttt{PerformanceNavigationTiming}.

    \item \textbf{Tempo di Inizializzazione e Rendering}: Calcolato dall'istante in cui i dati sono disponibili fino al completamento del rendering della heatmap. Per ogni libreria, questo evento è segnalato in modo specifico:
    \begin{itemize}
        \item \textbf{Leaflet}: Utilizza l'evento \texttt{layeradd} del plugin \texttt{Leaflet.heat} per rilevare l'aggiunta del layer heatmap alla mappa \cite{leaflet-heat}.
        \item \textbf{MapLibre GL JS}: Monitora l'evento \texttt{sourcedata} con controllo dello stato \texttt{loaded} per determinare quando i dati della sorgente sono completamente caricati \cite{maplibre-sourcedata}.
        \item \textbf{OpenLayers}: Sfrutta l'evento \texttt{postrender} del layer heatmap per identificare il completamento del rendering \cite{openlayers-heatmap}.
        \item \textbf{Deck.gl}: Utilizza il callback \texttt{onAfterRender} del componente per rilevare quando il rendering è stato completato \cite{deckgl-onafterrender}.
    \end{itemize}

    \item \textbf{Tempo Totale di Caricamento}: Rappresenta la somma del tempo di recupero dati e del tempo di inizializzazione e rendering, fornendo una metrica complessiva dell'esperienza utente.
\end{itemize}

Riassumendo, ogni iterazione di questo test viene eseguita in un \textit{iframe} isolato per evitare interferenze tra le diverse esecuzioni. Le metriche vengono raccolte lato client attraverso un evento personalizzato \texttt{benchmarkMetricsReady} che viene \textit{dispatchato} al completamento di ogni ciclo di \textit{rendering}. I dati raccolti vengono quindi aggregati e salvati in un file CSV per l'analisi successiva. Questo approccio metodico assicura che le misurazioni siano rappresentative delle reali prestazioni di ciascuna libreria in un contesto applicativo reale.


\subsection{Analisi sulla Correlazione delle Metriche}
\input{chapters/librerie-plot/grafico}

% commento grafico linee
Come illustrato in Figura \ref{fig:map_benchmark}, il grafico riassume le prestazioni delle librerie poste in analisi, in relazione alle metriche chiave di caricamento. Le linee presenti nei grafici delineano l'andamento dei dati rilevati e permettono di valutare in modo visivo i punti di forza e di debolezza di ciascuna soluzione.

Nello specifico, il grafico mette a confronto i seguenti dati rilevati:
    \begin{itemize}
        \item Tempo di Recupero dei Dati Geospaziali
        \item Tempo di Inizializzazione e Rendering della Mappa
        \item Tempo totale di Caricamento
    \end{itemize}
    

L'asse \textbf{verticale} rappresenta il tempo in millisecondi impiegato, mentre l'asse \textbf{orizzontale} categorizza la sequenza di rilevazioni ottenute.

Si osserva che le librerie Deck.gl e MapBox/MapLibre eccellono particolarmente nel tempo di inizializzazione e rendering, seguite da Leaflet, registrando i valori più bassi, il che le rende una scelta ottimale per scenari in cui tale aspetto è critico. Al contrario, la libreria OpenLayers mostra prestazioni meno competitive nello stesso ambito, suggerendo aree che richiedono potenziale ottimizzazione o che la rendono meno adatta per carichi di lavoro specifici.

È interessante notare come Leaflet e Deck.gl mostrino prestazioni simili e costantemente basse per il tempo di rendering e OpenLayers presenti una variabilità maggiore per il tempo totale di caricamento. Questo potrebbe indicare fattori architetturali comuni, diverse strategie di ottimizzazione, o impatti di terze parti meno controllabili.

L'analisi di questo grafico di confronto è fondamentale per orientare la scelta della libreria più adatta alle esigenze specifiche del progetto. Essa rivela non solo le performance assolute, ma anche le loro caratteristiche relative, permettendo di identificare la soluzione che meglio si allinea ai requisiti di velocità e reattività dell'applicazione finale.

% commento grafico xy
La Figura \ref{fig:map_xy_plot} mostra un grafico di dispersione tra il \textbf{tempo di rendering della mappa} (ms, ascisse) e il \textbf{tempo totale di caricamento lato client} (ms, ordinate). Ogni punto rappresenta un test singolo, con colori distinti per le librerie Leaflet, OpenLayers, MapLibre e Deck.gl.

\begin{itemize}
    \item \textbf{Limite inferiore:}  
    Nessuna libreria scende sotto circa $500$ ms di caricamento totale. La distribuzione lungo l'asse delle ascisse evidenzia i tempi della mappa e del componente \textit{Heatmap}, che costituiscono il totale (asse ordinate). Questo spiega l'assenza di punti sotto la diagonale $y=x$.  
    Deck.gl, Leaflet e MapLibre si avvicinano all'origine, indicando tempi di caricamento dei componenti Mappa e \textit{Heatmap} molto bassi, parametro rilevante nella scelta della libreria.

    \item \textbf{Ulteriori osservazioni:}  
    La vicinanza dei punti alla diagonale $y=x$ indica differenze ridotte tra rendering e caricamento totale. OpenLayers, pur avendo tempi maggiori di rendering, mostra solo un leggero incremento nel totale, suggerendo un impatto significativo del componente \textit{Heatmap}. L'ottimizzazione di quest'ultimo sposterebbe i punti verso il basso. Complessivamente, il grafico aiuta a identificare le librerie mediamente più rapide; solo successivamente si può analizzare in dettaglio l'efficienza del rendering e della \textit{Heatmap}.
\end{itemize}

In sintesi, l'analisi di questo grafico di correlazione fornisce \textit{insight} preziosi sui dati riguardanti la dipendenza del tempo di caricamento totale dalle performance di rendering.

\subsection*{Considerazioni sulla Validità e Interpretazione dei Risultati}
Pur avendo standardizzato il processo, le misurazioni di performance in un browser restano soggette a variabilità. Fattori come il carico di sistema, i processi in esecuzione, le estensioni del browser e la latenza di rete (per il recupero di tile e dati) possono influenzare i risultati.  
Le specifiche del sistema utilizzato sono riportate nel frammento di codice del Listing \ref{lst:inxi_output}.


\begin{listing}[H]
\caption{Output del comando \texttt{inxi}}
\label{lst:inxi_output}
\begin{minted}{bash}
$ inxi
CPU: 8-core AMD Ryzen 7 5800X (-MT MCP-) speed/min/max: 3384/556/4854 MHz
Kernel: 6.15.2-arch1-1 x86_64 Up: 2h 13m Mem: 8.44/15.52 GiB (54.4%)
Storage: 1.6 TiB (30.9% used) Procs: 403 Shell: Bash inxi: 3.3.38
\end{minted}
\end{listing}